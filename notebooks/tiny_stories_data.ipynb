{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27136acf-4b15-4918-9fbc-48ac4ac2fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399e6c16-757b-4d4d-ac59-617360e9ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from building_babel.tokenizers import LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import building_babel.model as bbm\n",
    "import torch.nn.functional as F\n",
    "from logging import basicConfig, INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ce0022-1872-489f-9449-645533da8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "basicConfig(level=INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7617814b-5b6f-429a-8836-d912381f9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/spott/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/df479c2caefda65cbb295f6f9d3e61b0be8d6593/data/train-00000-of-00004-2d5a1467fff1081b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2886db41-1d86-4aa4-a13f-e8e023ca8e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__name__:reloaded SentencePiece model from /Users/spott/Models/llama-2-tokenizer/tokenizer.model\n",
      "INFO:__name__:#words: 32000 - BOS ID: 1 - EOS ID: 2\n"
     ]
    }
   ],
   "source": [
    "lt = LlamaTokenizer(\"/Users/spott/Models/llama-2-tokenizer/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000ca6c2-a9c1-42cc-82a2-17fb9dda7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = torch.zeros((lt.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726efb07-9106-4248-93b7-06877665c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in tqdm(df['text']):\n",
    "#     bins += np.bincount(lt.encode(d,False, False), minlength=lt.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9262df-a3d0-43d5-b2fe-a4d355bb5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sibins = torch.argsort(bins, descending=True)\n",
    "# sbins = bins[sibins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3fa6ea-0c8d-475c-aa73-cdddd9786d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = [lt.sp_model.IdToPiece(i) for i in sibins.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf78788-1383-4a7a-b647-f7c283b45921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(bins.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d0878-32be-4591-8468-c715067e2a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba757cf0-a7f9-4335-8816-07f1db879101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6affbe15-fba6-4753-b608-47d1ddc23ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a big car named Dependable. He had a very important job. Dependable would take a family to the park every day. The family had a mom, dad, and a little girl named Lily. They all had a lot of love for each other.\\n\\nOne day, when they got to the park, they saw a big sign that said, \"Fun Race Today!\" The family was very excited. They knew that Dependable was very fast and could win the race. So, they decided to join the race.\\n\\nThe race started, and Dependable went very fast. The other cars tried to catch up, but Dependable was too quick. In the end, Dependable won the race! The family was so happy and proud of their car. They knew that their love for each other and their trust in Dependable made them win the race. And from that day on, they had even more fun at the park, knowing that they had the fastest and most dependable car around.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abe51bc-a128-4a8a-80ef-0d44999132d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df['text']\n",
    "        self.max_len = df['text'].apply(len).max()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tokenized = torch.tensor(lt.encode(self.df[i], True, True),  dtype=torch.int64)\n",
    "        pad = (0,self.max_len - len(tokenized))\n",
    "        return F.pad(tokenized, pad, \"constant\", 0) # we pad with 0s, but it really doesn't matter, because we have a stop token...\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x):\n",
    "    return torch.multinomial(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ed04ec-ae9d-48ec-8414-f2bc7898d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(t, deterministic=False):\n",
    "    seq = torch.tensor([[lt.bos_id]])\n",
    "    for i in range(18):\n",
    "        if deterministic:\n",
    "            next_token = t(seq)[:,-1].softmax(dim=-1).argmax(dim=-1).view(-1,1)\n",
    "        else:\n",
    "            next_token = sample(t(seq)[:,-1].softmax(dim=-1)).view(-1,1)\n",
    "        if next_token[0,-1] < 2:\n",
    "            break\n",
    "        seq = torch.concat([seq, next_token], dim=-1)\n",
    "    print(lt.decode(seq[:,1:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb1f1ff-f84d-46ae-bc7a-2b5961663a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = SimpleDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "036a34e4-b2bf-486a-9c09-de3346c3d19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5499"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda38f56-908b-473a-8653-197b53a1a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = bbm.TransformerConfig(128, 1, lt.n_words, head_dim=128, max_seq_len=5499)\n",
    "t = bbm.Transformer(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d02ab6-9a38-4344-b634-1f7dfc9a83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(sds, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d67cf75-3d18-491c-b127-4a90e21304f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(t.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2294e17-4d00-4cf8-981a-246fea39e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa6294bd512451eaebd5d44ea1373d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 256]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 128]), x.shape=torch.Size([10, 5498, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([10, 5498, 32000]), x.shape=torch.Size([10, 5498, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/spott/code/my_code/building_babel/notebooks/tiny_stories_data.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/spott/code/my_code/building_babel/notebooks/tiny_stories_data.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     out \u001b[39m=\u001b[39m t(b[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/spott/code/my_code/building_babel/notebooks/tiny_stories_data.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(out\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m), b[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,\u001b[39m1\u001b[39m:])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/spott/code/my_code/building_babel/notebooks/tiny_stories_data.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/spott/code/my_code/building_babel/notebooks/tiny_stories_data.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/spott/code/my_code/building_babel/notebooks/tiny_stories_data.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/code/my_code/building_babel/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/code/my_code/building_babel/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(i)\n",
    "    for b in tqdm(dl):\n",
    "        optim.zero_grad()\n",
    "        out = t(b[...,:-1])\n",
    "\n",
    "        loss = F.cross_entropy(out.transpose(1,2), b[...,1:])\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        generate(t, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 128]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 128]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 128]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 128]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 256]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 256]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 128]), x.shape=torch.Size([1, 1, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 1, 32000]), x.shape=torch.Size([1, 1, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 128]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 128]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 128]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 128]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 256]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 256]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 128]), x.shape=torch.Size([1, 2, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 2, 32000]), x.shape=torch.Size([1, 2, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 128]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 128]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 128]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 128]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 256]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 256]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 128]), x.shape=torch.Size([1, 3, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 3, 32000]), x.shape=torch.Size([1, 3, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 128]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 128]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 128]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 128]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 256]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 256]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 128]), x.shape=torch.Size([1, 4, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 4, 32000]), x.shape=torch.Size([1, 4, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 128]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 128]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 128]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 128]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 256]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 256]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 128]), x.shape=torch.Size([1, 5, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 5, 32000]), x.shape=torch.Size([1, 5, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 128]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 128]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 128]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 128]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 256]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 256]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 128]), x.shape=torch.Size([1, 6, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 6, 32000]), x.shape=torch.Size([1, 6, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 128]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 128]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 128]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 128]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 256]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 256]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 128]), x.shape=torch.Size([1, 7, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 7, 32000]), x.shape=torch.Size([1, 7, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 128]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 128]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 128]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 128]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 256]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 256]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 128]), x.shape=torch.Size([1, 8, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 8, 32000]), x.shape=torch.Size([1, 8, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 128]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 128]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 128]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 128]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 256]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 256]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 128]), x.shape=torch.Size([1, 9, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 9, 32000]), x.shape=torch.Size([1, 9, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 128]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 128]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 128]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 128]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 256]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 256]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 128]), x.shape=torch.Size([1, 10, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 10, 32000]), x.shape=torch.Size([1, 10, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 128]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 128]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 128]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 128]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 256]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 256]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 128]), x.shape=torch.Size([1, 11, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 11, 32000]), x.shape=torch.Size([1, 11, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 128]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 128]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 128]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 128]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 256]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 256]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 128]), x.shape=torch.Size([1, 12, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 12, 32000]), x.shape=torch.Size([1, 12, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 128]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 128]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 128]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 128]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 256]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 256]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 128]), x.shape=torch.Size([1, 13, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 13, 32000]), x.shape=torch.Size([1, 13, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 128]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 128]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 128]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 128]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 256]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 256]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 128]), x.shape=torch.Size([1, 14, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 14, 32000]), x.shape=torch.Size([1, 14, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 128]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 128]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 128]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 128]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 256]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 256]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 128]), x.shape=torch.Size([1, 15, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 15, 32000]), x.shape=torch.Size([1, 15, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 128]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 128]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 128]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 128]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 256]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 256]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 128]), x.shape=torch.Size([1, 16, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 16, 32000]), x.shape=torch.Size([1, 16, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 128]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 128]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 128]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 128]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 256]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 256]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 128]), x.shape=torch.Size([1, 17, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 17, 32000]), x.shape=torch.Size([1, 17, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n",
      "INFO:__name__:transformer block\n",
      "INFO:__name__:attention forward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 128]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 128]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 128]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:weights\n",
      "INFO:__name__:after rotary emb\n",
      "INFO:__name__:after attention\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 128]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([128, 128]), in_dim=128, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after attention stuff\n",
      "INFO:__name__:feedforward\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 256]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 256]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([256, 128]), in_dim=128, prev_in_dim=0, out_dim=256, prev_out_dim=0\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 128]), x.shape=torch.Size([1, 18, 256]), self.weight_splits[str(i)].shape=torch.Size([128, 256]), in_dim=256, prev_in_dim=0, out_dim=128, prev_out_dim=0\n",
      "INFO:__name__:after transformer block\n",
      "INFO:__name__:after layers\n",
      "INFO:__name__:after final norm\n",
      "INFO:__name__:in growable_linear\n",
      "INFO:__name__:building output\n",
      "INFO:__name__:0 output: torch.Size([1, 18, 32000]), x.shape=torch.Size([1, 18, 128]), self.weight_splits[str(i)].shape=torch.Size([32000, 128]), in_dim=128, prev_in_dim=0, out_dim=32000, prev_out_dim=0\n",
      "INFO:__name__:after final output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  scoreutch coup mantenalert externas intermediate orage Ribultan Scouseelijk rr alias']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        generate(t, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
